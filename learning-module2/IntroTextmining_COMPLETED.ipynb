{"cells":[{"cell_type":"markdown","metadata":{"id":"yCyuDv6ElAZf"},"source":["# Introduction to Textmining with NLTK\n","\n","A short introduction in data processing for textual data and some basic applications for sentiment analysis and"]},{"cell_type":"markdown","metadata":{"id":"BkZvhqMr_7jp"},"source":["# Basic Setup\n","\n","\n","Install nltk library for text processing and download some extensions that are required. Also, we install the wordcloud library for plotting our results as wordcloud."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yQhsMSw0lAZo"},"outputs":[],"source":["!pip install nltk\n","import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","\n","!pip install wordcloud"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qjYyB1uE_7js"},"outputs":[],"source":["# we import a series of specific functions from the nltk package for processing the texts.\n","from nltk.corpus import stopwords\n","from nltk.tokenize import RegexpTokenizer\n","from nltk.stem import WordNetLemmatizer\n","from nltk.stem import SnowballStemmer\n","from nltk import FreqDist\n","\n","# we import two functions from the library wordcloud to create word clouds; matplotlib.pyplot is used for plotting the wordloud\n","from wordcloud import WordCloud, STOPWORDS\n","import matplotlib.pyplot as plt\n","\n","# we import pandas for reading in files\n","import pandas as pd"]},{"cell_type":"markdown","metadata":{"id":"c8XRycrYlAZs"},"source":["## Read in the data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1xIqDR9DlAZs"},"outputs":[],"source":["corpus = pd.read_csv(\"https://github.com/casbdai/notebooks2023/raw/main/Module2/Textmining/fake_news.csv\")\n","corpus.head()"]},{"cell_type":"markdown","metadata":{"id":"clREJQkKlAZs"},"source":["We extract the first document and save it as an object text."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sYc54tyNlAZt"},"outputs":[],"source":["text = corpus[\"text\"][1]\n","print(text)"]},{"cell_type":"markdown","metadata":{"id":"7JWYdbmQlAZt"},"source":["## Pre-Processing Textual Data"]},{"cell_type":"markdown","metadata":{"id":"ssGwOsb6_7jv"},"source":["### Convert text to lower case:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N-0nNcOW_7jw"},"outputs":[],"source":["lower_text = text.lower()\n","print (lower_text)"]},{"cell_type":"markdown","metadata":{"id":"JkSpTRuC_7j0"},"source":["### Tokenize text\n","\n","Break down text into tokens, i.e, breaking the sentences into single words for analysis."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dX9pJ-0x_7j1"},"outputs":[],"source":["word_tokens = nltk.word_tokenize(lower_text)\n","print (word_tokens)"]},{"cell_type":"markdown","metadata":{"id":"fSE97tGG_7j3"},"source":["We need a better tokenizer also \"punctuation\" and \"numbers\" are retained as tokens. Also, very short words are translated into tokens.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F_SjwHcM_7j4"},"outputs":[],"source":["better_tokenizer = RegexpTokenizer(r'[a-zA-Z]{3,}')\n","\n","# [a-zA-Z] means that only letters are retained as tokens\n","# {3,} means that only tokens with at least three characters are retained"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rWDHnw5wlAZv"},"outputs":[],"source":["word_tokens = better_tokenizer.tokenize(lower_text)\n","print(word_tokens)"]},{"cell_type":"markdown","metadata":{"id":"EUvyYiX6_7j6"},"source":["## Remove stop words\n","\n","Remove irrelevant words using nltk stop words like is,the,a etc from the sentences as they donâ€™t carry any information."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nIymk1QOlAZw"},"outputs":[],"source":["stopword = stopwords.words('english')\n","stopword"]},{"cell_type":"markdown","metadata":{"id":"yZOlPzevlAZw"},"source":["For getting rid of stopwords, we must compare each token against the words in the stop words list. With can be easily done in a list comprehension. List comprehension are a common extension of \"for-loops\".\n","\n","A for loop that prints out every token:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GCjMf97BlAZw"},"outputs":[],"source":["for word in word_tokens:\n","    print(word)"]},{"cell_type":"markdown","metadata":{"id":"ixyraaB6lAZx"},"source":["Reformulating the for loop as a list comprehension. List comprehensions are considered to be very understandable and are thus used very frequently by pythonistas."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6EoK0NfSlAZx"},"outputs":[],"source":["[word for word in word_tokens]"]},{"cell_type":"markdown","metadata":{"id":"UCP8eHeZlAZx"},"source":["Extending our list comprehension such that only tokens are retained that are NOT on the stoplist."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2_kUc5Ge_7j7"},"outputs":[],"source":["clean_tokens = [word for word in word_tokens if word not in stopword]\n","print (clean_tokens)"]},{"cell_type":"markdown","metadata":{"id":"YWv-4wbB_7j9"},"source":["## Lemmatization / Stemming\n","\n","Stemming and Lemmatization\n","\n","Often we want to map the different forms of the same word to the same root word, e.g. \"walks\", \"walking\", \"walked\" should all be the same as \"walk\".\n","\n","The stemming and lemmatization process are hand-written rules written find the root word.\n","\n","- Stemming: Trying to shorten a word with simple cutoff rules\n","- Lemmatization: Trying to find the root word with linguistics rules"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AIUN03mP_7j-"},"outputs":[],"source":["wordnet_lemmatizer = WordNetLemmatizer()\n","lemmatized_tokens = [wordnet_lemmatizer.lemmatize(word) for word in clean_tokens]\n","print (lemmatized_tokens)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CNYExNQ0_7kA"},"outputs":[],"source":["snowball_stemmer = SnowballStemmer('english')\n","\n","stemmed_tokens = [snowball_stemmer.stem(word) for word in lemmatized_tokens]\n","print (stemmed_token)"]},{"cell_type":"markdown","metadata":{"id":"PM5dPmVK_7kC"},"source":["## Get word frequency\n","\n","Counting the most frequently used words in a textdocument"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"46OcQjjl_7kE"},"outputs":[],"source":["freq = FreqDist(stemmed_tokens)\n","print (freq.most_common(5))"]},{"cell_type":"markdown","metadata":{"id":"NJbPkULzlAZz"},"source":["## Create a Wordclout\n","\n","As a last step, we can now plot our results. The library wordcloud does all pre-processing steps under the hood."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AVtCk7_3lAZz"},"outputs":[],"source":["wordcloud = WordCloud(max_words=25, background_color=\"white\").generate(corpus[\"text\"][1])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1qjSnIm4lAZz"},"outputs":[],"source":["plt.imshow(wordcloud)\n","plt.axis(\"off\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"ighWdSB8lAZ0"},"source":["# Compare Fake and Real News\n","\n","## Fake News"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A3SFS_kolAZ0"},"outputs":[],"source":["fakenews = corpus.loc[corpus[\"label\"] == \"Fake\"].values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nU0864fhlAZ0"},"outputs":[],"source":["wordcloud = WordCloud(max_words=25, background_color=\"white\").generate(str(fakenews))\n","plt.imshow(wordcloud)\n","plt.axis(\"off\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"6p6dp_JIlAZ0"},"source":["## True News"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dgaCcqUzlAZ0"},"outputs":[],"source":["realnews = corpus.loc[corpus[\"label\"] == \"Real\"].values\n","\n","wordcloud = WordCloud(max_words=25, background_color=\"white\").generate(str(realnews))\n","plt.imshow(wordcloud)\n","plt.axis(\"off\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"M6VoToo9lAZ1"},"source":["# Very Basic Sentiment Analysis\n","\n","Using a dictionairy of positive and negative words, we can now perform a very basic sentiment analysis"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i2LlUTIVlAZ1"},"outputs":[],"source":["pos_sent = []\n","\n","[pos_sent.append(1) for word in stemmed_tokens if word in [\"correct\", \"good\", \"increas\"] ]\n","\n","sum(pos_sent)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4S4SZpZylAZ1"},"outputs":[],"source":["neg_sent = []\n","\n","[neg_sent.append(1) for word in stemmed_token if word in [\"virus\", \"infect\",\"gun\"] ]\n","\n","sum(neg_sent)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R1lYkZrPlAZ1"},"outputs":[],"source":["sum(pos_sent) - sum(neg_sent)"]}],"metadata":{"colab":{"provenance":[{"file_id":"1l3avI8VCGX3KQ5gyt2UMkafeGCa0UOkw","timestamp":1578350291282}]},"kernelspec":{"display_name":"venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.10"},"vscode":{"interpreter":{"hash":"9cfc3c7994f631dfc6a65b56363e87144dd9fa5c38ebff28a3247fb8dab8888e"}}},"nbformat":4,"nbformat_minor":0}